{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d3b0667e",
      "metadata": {},
      "source": [
        "# Bakaano-Hydro Full Workflow (Colab)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/confidence-duku/bakaano-hydro/blob/main/colab_full_workflow.ipynb)\n",
        "\n",
        "This notebook runs the end-to-end workflow:\n",
        "1. Input data preprocessing\n",
        "2. Runoff computation and routing\n",
        "3. Interactive exploration\n",
        "4. Training and evaluation\n",
        "5. Simulation/inference\n",
        "\n",
        "Before running, upload your basin shapefile and (optionally) GRDC/CSV station data to Google Drive.\n",
        "\n",
        "**Required user inputs before running:**\n",
        "- `study_area`: path to your basin shapefile (`.shp`)\n",
        "- choose one observed-data mode:\n",
        "  - `grdc_netcdf` path, or\n",
        "  - `csv_dir` + `lookup_csv` paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c0e4f17",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install package from GitHub\n",
        "!pip -q install \"bakaano-hydro[gpu] @ git+https://github.com/confidence-duku/bakaano-hydro.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77398424",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify GPU runtime in Colab\n",
        "import tensorflow as tf\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print('GPUs:', gpus)\n",
        "if not gpus:\n",
        "    raise RuntimeError('No GPU detected. In Colab, set Runtime -> Change runtime type -> GPU.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cd93094",
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c97da60",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# ------------------------------\n",
        "# User configuration\n",
        "# ------------------------------\n",
        "working_dir = Path('/content/drive/MyDrive/bakaano_workflow')\n",
        "study_area = Path('/content/drive/MyDrive/bakaano_workflow/shapes/study_area.shp')  # REQUIRED: user must set this\n",
        "\n",
        "# Optional observed streamflow inputs\n",
        "grdc_netcdf = Path('/content/drive/MyDrive/bakaano_workflow/data/GRDC.nc')  # REQUIRED for GRDC mode; set None for CSV mode\n",
        "csv_dir = None      # REQUIRED for CSV mode: Path('/content/drive/MyDrive/bakaano_workflow/data/station_csvs')\n",
        "lookup_csv = None   # REQUIRED for CSV mode: Path('/content/drive/MyDrive/bakaano_workflow/data/station_lookup.csv')\n",
        "\n",
        "# Core time windows\n",
        "prep_start = '2001-01-01'\n",
        "prep_end = '2010-12-31'\n",
        "train_start = '1981-01-01'\n",
        "train_end = '2020-12-31'\n",
        "val_start = '2001-01-01'\n",
        "val_end = '2010-12-31'\n",
        "sim_start = '1981-01-01'\n",
        "sim_end = '2020-12-31'\n",
        "\n",
        "# Model/data options\n",
        "climate_data_source = 'ERA5'   # ERA5, CHIRPS, or CHELSA\n",
        "routing_method = 'mfd'         # mfd, d8, or dinf\n",
        "\n",
        "# Training options\n",
        "batch_size = 32\n",
        "num_epochs = 300\n",
        "learning_rate = 1e-3\n",
        "loss_function = 'mse'\n",
        "area_normalize = True\n",
        "\n",
        "# Toggle heavy steps as needed\n",
        "RUN_PREPROCESS = True\n",
        "RUN_RUNOFF_ROUTING = True\n",
        "RUN_TRAIN = True\n",
        "RUN_EVAL = True\n",
        "RUN_SIM_GRDC = True\n",
        "RUN_SIM_POINTS = True\n",
        "\n",
        "working_dir.mkdir(parents=True, exist_ok=True)\n",
        "print('working_dir:', working_dir)\n",
        "print('study_area exists:', study_area.exists())\n",
        "print('grdc_netcdf exists:', grdc_netcdf.exists() if grdc_netcdf is not None else False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observed Data Mode (Required)\n",
        "\n",
        "Choose **exactly one** mode before running heavy steps:\n",
        "\n",
        "1. **GRDC mode**: set `grdc_netcdf=Path(...)`, and keep `csv_dir=None`, `lookup_csv=None`.\n",
        "2. **CSV mode**: set `grdc_netcdf=None`, and set both `csv_dir=Path(...)` and `lookup_csv=Path(...)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate required inputs and enforce exactly one observed-data mode\n",
        "if not study_area.exists():\n",
        "    raise FileNotFoundError(f'study_area not found: {study_area}')\n",
        "\n",
        "grdc_mode = grdc_netcdf is not None\n",
        "csv_mode = (csv_dir is not None) and (lookup_csv is not None)\n",
        "\n",
        "if grdc_mode == csv_mode:\n",
        "    raise ValueError(\n",
        "        'Choose exactly one observed-data mode:\\n'\n",
        "        \"  1) GRDC mode: set grdc_netcdf=Path(...), csv_dir=None, lookup_csv=None\\n\"\n",
        "        \"  2) CSV mode: set grdc_netcdf=None, csv_dir=Path(...), lookup_csv=Path(...)\"\n",
        "    )\n",
        "\n",
        "if grdc_mode and not grdc_netcdf.exists():\n",
        "    raise FileNotFoundError(f'grdc_netcdf not found: {grdc_netcdf}')\n",
        "\n",
        "if csv_mode:\n",
        "    if not csv_dir.exists():\n",
        "        raise FileNotFoundError(f'csv_dir not found: {csv_dir}')\n",
        "    if not lookup_csv.exists():\n",
        "        raise FileNotFoundError(f'lookup_csv not found: {lookup_csv}')\n",
        "\n",
        "print('Input validation passed.')\n",
        "print('Mode:', 'GRDC' if grdc_mode else 'CSV')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Download and preprocess input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if RUN_PREPROCESS:\n",
        "    from bakaano.tree_cover import TreeCover\n",
        "    from bakaano.ndvi import NDVI\n",
        "    from bakaano.dem import DEM\n",
        "    from bakaano.soil import Soil\n",
        "    from bakaano.alpha_earth import AlphaEarth\n",
        "    from bakaano.meteo import Meteo\n",
        "\n",
        "    # Tree cover\n",
        "    vf = TreeCover(\n",
        "        working_dir=str(working_dir),\n",
        "        study_area=str(study_area),\n",
        "        start_date=prep_start,\n",
        "        end_date=prep_end,\n",
        "    )\n",
        "    vf.get_tree_cover_data()\n",
        "\n",
        "    # NDVI\n",
        "    nd = NDVI(\n",
        "        working_dir=str(working_dir),\n",
        "        study_area=str(study_area),\n",
        "        start_date=prep_start,\n",
        "        end_date=prep_end,\n",
        "    )\n",
        "    nd.get_ndvi_data()\n",
        "\n",
        "    # DEM\n",
        "    dd = DEM(\n",
        "        working_dir=str(working_dir),\n",
        "        study_area=str(study_area),\n",
        "        local_data=False,\n",
        "        local_data_path=None,\n",
        "    )\n",
        "    dd.get_dem_data()\n",
        "\n",
        "    # Soil\n",
        "    sgd = Soil(\n",
        "        working_dir=str(working_dir),\n",
        "        study_area=str(study_area),\n",
        "    )\n",
        "    sgd.get_soil_data()\n",
        "\n",
        "    # AlphaEarth\n",
        "    ae = AlphaEarth(\n",
        "        working_dir=str(working_dir),\n",
        "        study_area=str(study_area),\n",
        "        start_date='2013-01-01',\n",
        "        end_date='2024-01-01',\n",
        "    )\n",
        "    ae.get_alpha_earth()\n",
        "\n",
        "    # Meteorology\n",
        "    cd = Meteo(\n",
        "        working_dir=str(working_dir),\n",
        "        study_area=str(study_area),\n",
        "        start_date=prep_start,\n",
        "        end_date=prep_end,\n",
        "        local_data=False,\n",
        "        data_source=climate_data_source,\n",
        "    )\n",
        "    cd.get_meteo_data()\n",
        "\n",
        "    print('Preprocessing complete.')\n",
        "else:\n",
        "    print('Skipping preprocessing step.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional quick plots after preprocessing\n",
        "# vf.plot_tree_cover(variable='tree_cover')\n",
        "# nd.plot_ndvi(interval_num=10)\n",
        "# dd.plot_dem()\n",
        "# sgd.plot_soil(variable='wilting_point')\n",
        "# ae.plot_alpha_earth('A35')\n",
        "# cd.plot_meteo(variable='tasmin', date='2006-12-01')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Compute runoff and route to river network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if RUN_RUNOFF_ROUTING:\n",
        "    from bakaano.veget import VegET\n",
        "\n",
        "    vg = VegET(\n",
        "        working_dir=str(working_dir),\n",
        "        study_area=str(study_area),\n",
        "        start_date=prep_start,\n",
        "        end_date=prep_end,\n",
        "        climate_data_source=climate_data_source,\n",
        "        routing_method=routing_method,\n",
        "    )\n",
        "    vg.compute_veget_runoff_route_flow()\n",
        "    print('Runoff and routing complete.')\n",
        "else:\n",
        "    print('Skipping runoff/routing step.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bakaano.plot_runoff import RoutedRunoff\n",
        "\n",
        "rr = RoutedRunoff(\n",
        "    working_dir=str(working_dir),\n",
        "    study_area=str(study_area),\n",
        ")\n",
        "\n",
        "# Update date to one available in your runoff output\n",
        "rr.map_routed_runoff(date='2010-01-03', vmax=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Interactive exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bakaano.runner import BakaanoHydro\n",
        "\n",
        "bk = BakaanoHydro(\n",
        "    working_dir=str(working_dir),\n",
        "    study_area=str(study_area),\n",
        "    climate_data_source=climate_data_source,\n",
        ")\n",
        "\n",
        "if grdc_netcdf is not None and grdc_netcdf.exists():\n",
        "    bk.explore_data_interactively(\n",
        "        start_date='1989-01-01',\n",
        "        end_date='1989-12-31',\n",
        "        grdc_netcdf=str(grdc_netcdf),\n",
        "    )\n",
        "else:\n",
        "    print('Skipping explore_data_interactively (no GRDC NetCDF path set).')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if RUN_TRAIN:\n",
        "    if grdc_netcdf is not None and grdc_netcdf.exists():\n",
        "        bk.train_streamflow_model(\n",
        "            train_start=train_start,\n",
        "            train_end=train_end,\n",
        "            grdc_netcdf=str(grdc_netcdf),\n",
        "            batch_size=batch_size,\n",
        "            num_epochs=num_epochs,\n",
        "            learning_rate=learning_rate,\n",
        "            loss_function=loss_function,\n",
        "            lr_schedule='cosine',\n",
        "            warmup_epochs=5,\n",
        "            min_learning_rate=1e-5,\n",
        "            routing_method=routing_method,\n",
        "            area_normalize=area_normalize,\n",
        "        )\n",
        "    else:\n",
        "        # CSV-based training path\n",
        "        bk.train_streamflow_model(\n",
        "            train_start=train_start,\n",
        "            train_end=train_end,\n",
        "            grdc_netcdf=None,\n",
        "            batch_size=batch_size,\n",
        "            num_epochs=num_epochs,\n",
        "            learning_rate=learning_rate,\n",
        "            loss_function=loss_function,\n",
        "            lr_schedule='cosine',\n",
        "            warmup_epochs=5,\n",
        "            min_learning_rate=1e-5,\n",
        "            routing_method=routing_method,\n",
        "            area_normalize=area_normalize,\n",
        "            csv_dir=str(csv_dir),\n",
        "            lookup_csv=str(lookup_csv),\n",
        "            id_col='id',\n",
        "            lat_col='latitude',\n",
        "            lon_col='longitude',\n",
        "            date_col='date',\n",
        "            discharge_col='discharge',\n",
        "            file_pattern='{id}.csv',\n",
        "        )\n",
        "else:\n",
        "    print('Skipping training.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = working_dir / 'models' / 'bakaano_model.keras'\n",
        "print('model_path exists:', model_path.exists())\n",
        "\n",
        "if RUN_EVAL and model_path.exists():\n",
        "    if grdc_netcdf is not None and grdc_netcdf.exists():\n",
        "        bk.evaluate_streamflow_model_interactively(\n",
        "            model_path=str(model_path),\n",
        "            val_start=val_start,\n",
        "            val_end=val_end,\n",
        "            grdc_netcdf=str(grdc_netcdf),\n",
        "            routing_method=routing_method,\n",
        "            area_normalize=area_normalize,\n",
        "        )\n",
        "    else:\n",
        "        bk.evaluate_streamflow_model_interactively(\n",
        "            model_path=str(model_path),\n",
        "            val_start=val_start,\n",
        "            val_end=val_end,\n",
        "            grdc_netcdf=None,\n",
        "            routing_method=routing_method,\n",
        "            area_normalize=area_normalize,\n",
        "            csv_dir=str(csv_dir),\n",
        "            lookup_csv=str(lookup_csv),\n",
        "            id_col='id',\n",
        "            lat_col='latitude',\n",
        "            lon_col='longitude',\n",
        "            date_col='date',\n",
        "            discharge_col='discharge',\n",
        "            file_pattern='{id}.csv',\n",
        "        )\n",
        "else:\n",
        "    print('Skipping evaluation.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Simulate streamflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if model_path.exists() and RUN_SIM_GRDC:\n",
        "    if grdc_netcdf is not None and grdc_netcdf.exists():\n",
        "        bk.simulate_grdc_csv_stations(\n",
        "            model_path=str(model_path),\n",
        "            sim_start=sim_start,\n",
        "            sim_end=sim_end,\n",
        "            grdc_netcdf=str(grdc_netcdf),\n",
        "            routing_method=routing_method,\n",
        "            area_normalize=area_normalize,\n",
        "        )\n",
        "    else:\n",
        "        bk.simulate_grdc_csv_stations(\n",
        "            model_path=str(model_path),\n",
        "            sim_start=sim_start,\n",
        "            sim_end=sim_end,\n",
        "            grdc_netcdf=None,\n",
        "            routing_method=routing_method,\n",
        "            area_normalize=area_normalize,\n",
        "            csv_dir=str(csv_dir),\n",
        "            lookup_csv=str(lookup_csv),\n",
        "            id_col='id',\n",
        "            lat_col='latitude',\n",
        "            lon_col='longitude',\n",
        "            date_col='date',\n",
        "            discharge_col='discharge',\n",
        "            file_pattern='{id}.csv',\n",
        "        )\n",
        "else:\n",
        "    print('Skipping station simulation.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Arbitrary coordinate simulation\n",
        "if model_path.exists() and RUN_SIM_POINTS:\n",
        "    bk.simulate_streamflow(\n",
        "        model_path=str(model_path),\n",
        "        sim_start='1981-01-01',\n",
        "        sim_end='1990-12-31',\n",
        "        latlist=[13.8, 13.9],\n",
        "        lonlist=[3.0, 4.0],\n",
        "        routing_method=routing_method,\n",
        "        area_normalize=area_normalize,\n",
        "    )\n",
        "else:\n",
        "    print('Skipping point simulation.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect generated prediction files\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "pred_files = sorted(glob.glob(str(working_dir / 'predicted_streamflow_data' / '*.csv')))\n",
        "print('Prediction files:', len(pred_files))\n",
        "if pred_files:\n",
        "    print('Example:', pred_files[0])\n",
        "    df = pd.read_csv(pred_files[0])\n",
        "    display(df.head())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
