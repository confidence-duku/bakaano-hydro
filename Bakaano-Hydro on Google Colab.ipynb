{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3b0667e",
   "metadata": {},
   "source": [
    "# Bakaano-Hydro Full Workflow (Colab)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/confidence-duku/bakaano-hydro/blob/main/Bakaano-Hydro%20on%20Google%20Colab.ipynb)\n",
    "\n",
    "This notebook runs the end-to-end workflow:\n",
    "1. Input data preprocessing\n",
    "2. Runoff computation and routing\n",
    "3. Interactive exploration\n",
    "4. Training and evaluation\n",
    "5. Simulation/inference\n",
    "\n",
    "Before running, upload your basin shapefile and (optionally) GRDC/CSV station data to Google Drive.\n",
    "\n",
    "**Required user inputs before running:**\n",
    "- `study_area`: path to your basin shapefile (`.shp`)\n",
    "- choose one observed-data mode:\n",
    "  - `grdc_netcdf` path, or\n",
    "  - `csv_dir` + `lookup_csv` paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gpu-runtime-note",
   "metadata": {},
   "source": [
    "## Set Colab Runtime to GPU\n",
    "\n",
    "Before running install/training cells:\n",
    "1. Go to **Runtime -> Change runtime type**.\n",
    "2. Set **Hardware accelerator** to **GPU**.\n",
    "3. Click **Save**.\n",
    "\n",
    "Then run the next GPU check cell. If it reports no GPU, restart runtime and try again.\n",
    "\n",
    "\n",
    "For this notebook, we use TensorFlow GPU, so the install cell removes `torch` packages from the current runtime before installing Bakaano.\n",
    "\n",
    "If you need PyTorch later, use a separate Colab runtime/session for PyTorch workloads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0e4f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q uninstall -y torch torchvision torchaudio\n",
    "!pip -q install \"bakaano-hydro[gpu] @ git+https://github.com/confidence-duku/bakaano-hydro.git\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77398424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print('GPUs:', gpus)\n",
    "if not gpus:\n",
    "    raise RuntimeError('No GPU detected. In Colab, set Runtime -> Change runtime type -> GPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd93094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c97da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "shapefile_name = ' '\n",
    "grdc_filename = ' '\n",
    "\n",
    "working_dir_drive = Path('/content/drive/MyDrive/bakaano_workflow')\n",
    "working_dir_local = Path('/content/bakaano_workflow')\n",
    "working_dir = working_dir_drive\n",
    "study_area = working_dir_drive / 'shapes' / shapefile_name\n",
    "\n",
    "grdc_netcdf = working_dir_drive / 'data' / grdc_filename\n",
    "csv_dir = None\n",
    "lookup_csv = None\n",
    "\n",
    "climate_data_source = 'ERA5'\n",
    "routing_method = 'mfd'\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 300\n",
    "learning_rate = 1e-3\n",
    "loss_function = 'mse'\n",
    "area_normalize = True\n",
    "model_overwrite = True\n",
    "\n",
    "RUN_PREPROCESS = True\n",
    "RUN_RUNOFF_ROUTING = True\n",
    "RUN_TRAIN = True\n",
    "RUN_EVAL = True\n",
    "RUN_SIM_GRDC = True\n",
    "RUN_SIM_POINTS = True\n",
    "\n",
    "working_dir_drive.mkdir(parents=True, exist_ok=True)\n",
    "(working_dir_drive / 'shapes').mkdir(parents=True, exist_ok=True)\n",
    "(working_dir_drive / 'data').mkdir(parents=True, exist_ok=True)\n",
    "working_dir_local.mkdir(parents=True, exist_ok=True)\n",
    "(working_dir_local / 'shapes').mkdir(parents=True, exist_ok=True)\n",
    "(working_dir_local / 'data').mkdir(parents=True, exist_ok=True)\n",
    "print('working_dir (drive):', working_dir_drive)\n",
    "print('working_dir (local):', working_dir_local)\n",
    "print('study_area exists:', study_area.exists())\n",
    "print('grdc_netcdf exists:', grdc_netcdf.exists() if grdc_netcdf is not None else False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6b3e2c",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set values in the next code cell:\n",
    "- `shapefile_name`: shapefile filename in `.../bakaano_workflow/shapes/`\n",
    "- `grdc_filename`: GRDC NetCDF filename in `.../bakaano_workflow/data/` (GRDC mode)\n",
    "- `csv_dir` + `lookup_csv` (CSV mode)\n",
    "- run toggles: `RUN_PREPROCESS`, `RUN_RUNOFF_ROUTING`, `RUN_TRAIN`, `RUN_EVAL`, `RUN_SIM_GRDC`, `RUN_SIM_POINTS`\n",
    "\n",
    "## Observed Data Mode (Required)\n",
    "\n",
    "Choose exactly one mode before running heavy steps:\n",
    "1. **GRDC mode**: set `grdc_netcdf=Path(...)`, keep `csv_dir=None`, `lookup_csv=None`.\n",
    "2. **CSV mode**: set `grdc_netcdf=None`, and set both `csv_dir=Path(...)` and `lookup_csv=Path(...)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce1f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not study_area.exists():\n",
    "    raise FileNotFoundError(f'study_area not found: {study_area}')\n",
    "\n",
    "grdc_mode = grdc_netcdf is not None\n",
    "csv_mode = (csv_dir is not None) and (lookup_csv is not None)\n",
    "\n",
    "if grdc_mode == csv_mode:\n",
    "    raise ValueError(\n",
    "        'Choose exactly one observed-data mode:\\n'\n",
    "        \"  1) GRDC mode: set grdc_netcdf=Path(...), csv_dir=None, lookup_csv=None\\n\"\n",
    "        \"  2) CSV mode: set grdc_netcdf=None, csv_dir=Path(...), lookup_csv=Path(...)\"\n",
    "    )\n",
    "\n",
    "if grdc_mode and not grdc_netcdf.exists():\n",
    "    raise FileNotFoundError(f'grdc_netcdf not found: {grdc_netcdf}')\n",
    "\n",
    "if csv_mode:\n",
    "    if not csv_dir.exists():\n",
    "        raise FileNotFoundError(f'csv_dir not found: {csv_dir}')\n",
    "    if not lookup_csv.exists():\n",
    "        raise FileNotFoundError(f'lookup_csv not found: {lookup_csv}')\n",
    "\n",
    "print('Input validation passed.')\n",
    "print('Mode:', 'GRDC' if grdc_mode else 'CSV')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4381731",
   "metadata": {},
   "source": [
    "## 1) Download and preprocess input data\n",
    "\n",
    "This stage prepares all static and climate inputs under `working_dir`. Run this once per basin (or when basin/resolution/time range changes).\n",
    "\n",
    "Recommended order used here:\n",
    "1. DEM first (creates the reference grid at `elevation/dem_clipped.tif`)\n",
    "2. Tree cover\n",
    "3. NDVI\n",
    "4. Soil\n",
    "5. AlphaEarth\n",
    "6. Meteorology\n",
    "\n",
    "Dataset availability notes:\n",
    "- Tree cover (MODIS VCF): 2001 onward\n",
    "- NDVI (MODIS 16-day): 2001 onward\n",
    "- AlphaEarth embeddings: 2017 onward\n",
    "\n",
    "Expected outputs:\n",
    "- `elevation/`, `vcf/`, `ndvi/`, `soil/`, `alpha_earth/` folders\n",
    "- climate NetCDF files under your selected meteorology source folder\n",
    "\n",
    "\n",
    "Resume behavior:\n",
    "- If a previous run was interrupted, rerunning this section reuses existing files and processes missing pieces before downloading more data.\n",
    "- For ERA5/NDVI/Tree cover, missing-date checks are applied so only missing raw timesteps are downloaded when possible.\n",
    "\n",
    "If Earth Engine authentication is requested, sign in and paste the code back into Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e94ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "ee.Authenticate(auth_mode=\"notebook\")   # forces link + paste code flow\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d20ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_PREPROCESS:\n",
    "    from bakaano.dem import DEM\n",
    "    from bakaano.tree_cover import TreeCover\n",
    "    from bakaano.ndvi import NDVI\n",
    "    from bakaano.soil import Soil\n",
    "    from bakaano.alpha_earth import AlphaEarth\n",
    "    from bakaano.meteo import Meteo\n",
    "\n",
    "    dd = DEM(\n",
    "        working_dir=str(working_dir),\n",
    "        study_area=str(study_area),\n",
    "        local_data=False,\n",
    "        local_data_path=None,\n",
    "    )\n",
    "    dd.get_dem_data()\n",
    "\n",
    "    vf = TreeCover(\n",
    "        working_dir=str(working_dir),\n",
    "        study_area=str(study_area),\n",
    "        start_date='2001-01-01',\n",
    "        end_date='2010-12-31',\n",
    "    )\n",
    "    vf.get_tree_cover_data()\n",
    "\n",
    "    nd = NDVI(\n",
    "        working_dir=str(working_dir),\n",
    "        study_area=str(study_area),\n",
    "        start_date='2001-01-01',\n",
    "        end_date='2010-12-31',\n",
    "    )\n",
    "    nd.get_ndvi_data()\n",
    "\n",
    "    sgd = Soil(\n",
    "        working_dir=str(working_dir),\n",
    "        study_area=str(study_area),\n",
    "    )\n",
    "    sgd.get_soil_data()\n",
    "\n",
    "    ae = AlphaEarth(\n",
    "        working_dir=str(working_dir),\n",
    "        study_area=str(study_area),\n",
    "        start_date='2013-01-01',\n",
    "        end_date='2024-01-01',\n",
    "    )\n",
    "    ae.get_alpha_earth()\n",
    "\n",
    "    cd = Meteo(\n",
    "        working_dir=str(working_dir_local),\n",
    "        study_area=str(study_area),\n",
    "        start_date='2001-01-01',\n",
    "        end_date='2010-12-31',\n",
    "        local_data=False,\n",
    "        data_source=climate_data_source,\n",
    "    )\n",
    "    cd.get_meteo_data()\n",
    "    src_climate = working_dir_local / climate_data_source\n",
    "    dst_climate = working_dir_drive / climate_data_source\n",
    "    if src_climate.exists():\n",
    "        shutil.copytree(src_climate, dst_climate, dirs_exist_ok=True)\n",
    "        print(f'Synced climate outputs to Drive: {dst_climate}')\n",
    "\n",
    "    print('Preprocessing complete.')\n",
    "else:\n",
    "    print('Skipping preprocessing step.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e992bd2e",
   "metadata": {},
   "source": [
    "### Optional quick plots\n",
    "\n",
    "Run any of these in a new code cell if needed:\n",
    "- `vf.plot_tree_cover(variable='tree_cover')`\n",
    "- `nd.plot_ndvi(interval_num=10)`\n",
    "- `dd.plot_dem()`\n",
    "- `sgd.plot_soil(variable='wilting_point')`\n",
    "- `ae.plot_alpha_earth('A35')`\n",
    "- `cd.plot_meteo(variable='tasmin', date='2006-12-01')`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba5c85e",
   "metadata": {},
   "source": [
    "## 2) Compute runoff and route to river network\n",
    "\n",
    "This stage runs VegET runoff and routing. For speed, it copies required inputs from Drive to local disk (`/content`), computes locally, then syncs outputs back to Drive.\n",
    "\n",
    "Expected outputs:\n",
    "- `runoff_output/wacc_sparse_arrays.pkl`\n",
    "- routed runoff rasters and diagnostics in `runoff_output/`\n",
    "\n",
    "Resume behavior:\n",
    "- If interrupted, rerun this section; checkpoint files allow continuation instead of starting over.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca5a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_RUNOFF_ROUTING:\n",
    "    from bakaano.veget import VegET\n",
    "\n",
    "    for folder in ['shapes', 'elevation', 'soil', 'ndvi', 'vcf', 'alpha_earth', climate_data_source]:\n",
    "        src = working_dir_drive / folder\n",
    "        dst = working_dir_local / folder\n",
    "        if src.exists():\n",
    "            shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "    local_study_area = working_dir_local / 'shapes' / shapefile_name\n",
    "\n",
    "    vg = VegET(\n",
    "        working_dir=str(working_dir_local),\n",
    "        study_area=str(local_study_area),\n",
    "        start_date='2001-01-01',\n",
    "        end_date='2010-12-31',\n",
    "        climate_data_source=climate_data_source,\n",
    "        routing_method=routing_method,\n",
    "    )\n",
    "    vg.compute_veget_runoff_route_flow()\n",
    "    for folder in ['runoff_output', 'catchment']:\n",
    "        src = working_dir_local / folder\n",
    "        dst = working_dir_drive / folder\n",
    "        if src.exists():\n",
    "            shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "    print('Runoff and routing complete.')\n",
    "else:\n",
    "    print('Skipping runoff/routing step.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bf49cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bakaano.plot_runoff import RoutedRunoff\n",
    "\n",
    "rr = RoutedRunoff(\n",
    "    working_dir=str(working_dir),\n",
    "    study_area=str(study_area),\n",
    ")\n",
    "\n",
    "rr.map_routed_runoff(date='2010-01-03', vmax=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bdbb93",
   "metadata": {},
   "source": [
    "## 3) Interactive exploration\n",
    "\n",
    "Use this section to inspect routed runoff and basin behavior before training.\n",
    "\n",
    "Typical checks:\n",
    "- visualize routed runoff time slices\n",
    "- verify station alignment and basin coverage\n",
    "- confirm outputs exist before model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8732b41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bakaano.runner import BakaanoHydro\n",
    "\n",
    "bk = BakaanoHydro(\n",
    "    working_dir=str(working_dir),\n",
    "    study_area=str(study_area),\n",
    "    climate_data_source=climate_data_source,\n",
    ")\n",
    "\n",
    "if grdc_netcdf is not None and grdc_netcdf.exists():\n",
    "    bk.explore_data_interactively(\n",
    "        start_date='1989-01-01',\n",
    "        end_date='1989-12-31',\n",
    "        grdc_netcdf=str(grdc_netcdf),\n",
    "    )\n",
    "else:\n",
    "    print('Skipping explore_data_interactively (no GRDC NetCDF path set).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routed runoff timeseries\n",
    "\n",
    "Plot routed runoff time series at one or more stations.\n",
    "\n",
    "Instructions:\n",
    "- Set `station_ids` below to IDs available in your GRDC NetCDF or lookup CSV.\n",
    "- Keep the date range inside your runoff output period.\n",
    "- If you do not have observed-data metadata loaded, use the lat/lon example in `rr.plot_routed_runoff_timeseries(...)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_ids = ['6203100']  # <- replace with your station id(s)\n",
    "\n",
    "if grdc_netcdf is not None and grdc_netcdf.exists():\n",
    "    rr.plot_routed_runoff_timeseries(\n",
    "        start_date='2010-01-01',\n",
    "        end_date='2010-12-31',\n",
    "        station_id=station_ids,\n",
    "        grdc_netcdf=str(grdc_netcdf),\n",
    "    )\n",
    "elif lookup_csv is not None and lookup_csv.exists():\n",
    "    rr.plot_routed_runoff_timeseries(\n",
    "        start_date='2010-01-01',\n",
    "        end_date='2010-12-31',\n",
    "        station_id=station_ids,\n",
    "        lookup_csv=str(lookup_csv),\n",
    "    )\n",
    "else:\n",
    "    print('Set grdc_netcdf or lookup_csv, or call rr.plot_routed_runoff_timeseries with lat/lon directly.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ac3e76",
   "metadata": {},
   "source": [
    "## 4) Train model\n",
    "\n",
    "This section trains the Bakaano model using prepared runoff/features and observed streamflow data.\n",
    "\n",
    "Training control:\n",
    "- `model_overwrite=True`: start a fresh model\n",
    "- `model_overwrite=False`: load existing model (if present) and continue training\n",
    "\n",
    "Expected output:\n",
    "- `models/bakaano_model.keras`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93e3503",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_TRAIN:\n",
    "    if grdc_netcdf is not None and grdc_netcdf.exists():\n",
    "        bk.train_streamflow_model(\n",
    "            train_start='1981-01-01',\n",
    "            train_end='2020-12-31',\n",
    "            grdc_netcdf=str(grdc_netcdf),\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            loss_function=loss_function,\n",
    "            lr_schedule='cosine',\n",
    "            warmup_epochs=5,\n",
    "            min_learning_rate=1e-5,\n",
    "            routing_method=routing_method,\n",
    "            area_normalize=area_normalize,\n",
    "            model_overwrite=model_overwrite,\n",
    "        )\n",
    "    else:\n",
    "        bk.train_streamflow_model(\n",
    "            train_start='1981-01-01',\n",
    "            train_end='2020-12-31',\n",
    "            grdc_netcdf=None,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            loss_function=loss_function,\n",
    "            lr_schedule='cosine',\n",
    "            warmup_epochs=5,\n",
    "            min_learning_rate=1e-5,\n",
    "            routing_method=routing_method,\n",
    "            area_normalize=area_normalize,\n",
    "            model_overwrite=model_overwrite,\n",
    "            csv_dir=str(csv_dir),\n",
    "            lookup_csv=str(lookup_csv),\n",
    "            id_col='id',\n",
    "            lat_col='latitude',\n",
    "            lon_col='longitude',\n",
    "            date_col='date',\n",
    "            discharge_col='discharge',\n",
    "            file_pattern='{id}.csv',\n",
    "        )\n",
    "else:\n",
    "    print('Skipping training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33df53b",
   "metadata": {},
   "source": [
    "## 5) Evaluate model\n",
    "\n",
    "This section evaluates a saved model against observed data for held-out periods/stations.\n",
    "\n",
    "Expected outputs:\n",
    "- evaluation metrics\n",
    "- comparison plots/figures for observed vs simulated flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c221a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = working_dir / 'models' / 'bakaano_model.keras'\n",
    "print('model_path exists:', model_path.exists())\n",
    "\n",
    "if RUN_EVAL and model_path.exists():\n",
    "    if grdc_netcdf is not None and grdc_netcdf.exists():\n",
    "        bk.evaluate_streamflow_model_interactively(\n",
    "            model_path=str(model_path),\n",
    "            val_start='2001-01-01',\n",
    "            val_end='2010-12-31',\n",
    "            grdc_netcdf=str(grdc_netcdf),\n",
    "            routing_method=routing_method,\n",
    "            area_normalize=area_normalize,\n",
    "        )\n",
    "    else:\n",
    "        bk.evaluate_streamflow_model_interactively(\n",
    "            model_path=str(model_path),\n",
    "            val_start='2001-01-01',\n",
    "            val_end='2010-12-31',\n",
    "            grdc_netcdf=None,\n",
    "            routing_method=routing_method,\n",
    "            area_normalize=area_normalize,\n",
    "            csv_dir=str(csv_dir),\n",
    "            lookup_csv=str(lookup_csv),\n",
    "            id_col='id',\n",
    "            lat_col='latitude',\n",
    "            lon_col='longitude',\n",
    "            date_col='date',\n",
    "            discharge_col='discharge',\n",
    "            file_pattern='{id}.csv',\n",
    "        )\n",
    "else:\n",
    "    print('Skipping evaluation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5041ce1",
   "metadata": {},
   "source": [
    "## 6) Simulate streamflow\n",
    "\n",
    "Use a trained model to run simulation/inference for GRDC mode or point-based mode.\n",
    "\n",
    "Run notes:\n",
    "- requires `models/bakaano_model.keras`\n",
    "- choose either GRDC simulation or point simulation based on your observed-data mode\n",
    "\n",
    "Expected outputs:\n",
    "- simulated discharge series and saved prediction artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d8bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_path.exists() and RUN_SIM_GRDC:\n",
    "    if grdc_netcdf is not None and grdc_netcdf.exists():\n",
    "        bk.simulate_grdc_csv_stations(\n",
    "            model_path=str(model_path),\n",
    "            sim_start='1981-01-01',\n",
    "            sim_end='2020-12-31',\n",
    "            grdc_netcdf=str(grdc_netcdf),\n",
    "            routing_method=routing_method,\n",
    "            area_normalize=area_normalize,\n",
    "        )\n",
    "    else:\n",
    "        bk.simulate_grdc_csv_stations(\n",
    "            model_path=str(model_path),\n",
    "            sim_start='1981-01-01',\n",
    "            sim_end='2020-12-31',\n",
    "            grdc_netcdf=None,\n",
    "            routing_method=routing_method,\n",
    "            area_normalize=area_normalize,\n",
    "            csv_dir=str(csv_dir),\n",
    "            lookup_csv=str(lookup_csv),\n",
    "            id_col='id',\n",
    "            lat_col='latitude',\n",
    "            lon_col='longitude',\n",
    "            date_col='date',\n",
    "            discharge_col='discharge',\n",
    "            file_pattern='{id}.csv',\n",
    "        )\n",
    "else:\n",
    "    print('Skipping station simulation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36cb632",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_path.exists() and RUN_SIM_POINTS:\n",
    "    bk.simulate_streamflow(\n",
    "        model_path=str(model_path),\n",
    "        sim_start='1981-01-01',\n",
    "        sim_end='1990-12-31',\n",
    "        latlist=[13.8, 13.9],\n",
    "        lonlist=[3.0, 4.0],\n",
    "        routing_method=routing_method,\n",
    "        area_normalize=area_normalize,\n",
    "    )\n",
    "else:\n",
    "    print('Skipping point simulation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5063c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "pred_files = sorted(glob.glob(str(working_dir / 'predicted_streamflow_data' / '*.csv')))\n",
    "print('Prediction files:', len(pred_files))\n",
    "if pred_files:\n",
    "    print('Example:', pred_files[0])\n",
    "    df = pd.read_csv(pred_files[0])\n",
    "    display(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
