{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3b0667e",
   "metadata": {},
   "source": [
    "# Bakaano-Hydro Full Workflow (Colab)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/confidence-duku/bakaano-hydro/blob/main/Bakaano-Hydro%20on%20Google%20Colab.ipynb)\n",
    "\n",
    "This notebook runs the end-to-end workflow:\n",
    "1. Input data preprocessing\n",
    "2. Runoff computation and routing\n",
    "3. Interactive exploration\n",
    "4. Training and evaluation\n",
    "5. Simulation/inference\n",
    "\n",
    "Before running, upload your basin shapefile and (optionally) GRDC/CSV station data to Google Drive.\n",
    "\n",
    "**Required user inputs before running:**\n",
    "- `study_area`: path to your basin shapefile (`.shp`)\n",
    "- choose one observed-data mode:\n",
    "  - `grdc_netcdf` path, or\n",
    "  - `csv_dir` + `lookup_csv` paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gpu-runtime-note",
   "metadata": {},
   "source": [
    "## Set Colab Runtime to GPU\n",
    "\n",
    "Before running install/training cells:\n",
    "1. Go to **Runtime -> Change runtime type**.\n",
    "2. Set **Hardware accelerator** to **GPU**.\n",
    "3. Click **Save**.\n",
    "\n",
    "Then run the next GPU check cell. If it reports no GPU, restart runtime and try again.\n",
    "\n",
    "\n",
    "For this notebook, we use TensorFlow GPU, so the install cell removes `torch` packages from the current runtime before installing Bakaano.\n",
    "\n",
    "If you need PyTorch later, use a separate Colab runtime/session for PyTorch workloads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0e4f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q uninstall -y torch torchvision torchaudio\n",
    "!pip -q install \"bakaano-hydro[gpu] @ git+https://github.com/confidence-duku/bakaano-hydro.git\"\n",
    "!pip -q install h5netcdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77398424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print('GPUs:', gpus)\n",
    "if not gpus:\n",
    "    raise RuntimeError('No GPU detected. In Colab, set Runtime -> Change runtime type -> GPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd93094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012541a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Inputs (edit this cell only)\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1) Workspace and basin shapefile\n",
    "folder_name = 'bakaano_workflow'\n",
    "shapefile_name = 'your_basin.shp'  # must exist in MyDrive/<folder_name>/shapes/\n",
    "\n",
    "# 2) Observed-data mode (choose one): 'GRDC' or 'CSV'\n",
    "OBSERVED_DATA_MODE = 'GRDC'\n",
    "\n",
    "# GRDC mode input (used when OBSERVED_DATA_MODE='GRDC')\n",
    "grdc_filename = 'your_grdc_data.nc'  # file in MyDrive/<folder_name>/data/\n",
    "\n",
    "# CSV mode inputs (used when OBSERVED_DATA_MODE='CSV')\n",
    "csv_dir_name = 'csv_timeseries'      # folder in MyDrive/<folder_name>/data/\n",
    "lookup_csv_name = 'station_lookup.csv'  # file in MyDrive/<folder_name>/data/\n",
    "\n",
    "# 3) Global model settings\n",
    "climate_data_source = 'ERA5'  # ERA5, CHIRPS, CHELSA\n",
    "routing_method = 'mfd'        # mfd, d8, dinf\n",
    "\n",
    "# 4) Run toggles\n",
    "RUN_PREPROCESS = True\n",
    "RUN_RUNOFF_ROUTING = True\n",
    "RUN_TRAIN = True\n",
    "RUN_EVAL = True\n",
    "RUN_SIM_GRDC = True\n",
    "RUN_SIM_POINTS = True\n",
    "\n",
    "# 5) Dates (single shared window for beginner workflow)\n",
    "# These are reused by preprocessing, runoff routing, interactive exploration, and AlphaEarth.\n",
    "WORKFLOW_START_DATE = '2001-01-01'\n",
    "WORKFLOW_END_DATE = '2010-12-31'\n",
    "\n",
    "# Derived dates (kept explicit so downstream cells stay readable)\n",
    "TRAIN_START_DATE = '2001-01-01'\n",
    "TRAIN_END_DATE = '2020-12-31'\n",
    "\n",
    "EVAL_START_DATE = '2001-01-01'\n",
    "EVAL_END_DATE = '2020-12-31'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171feb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Advanced Settings (optional; beginners can skip editing this cell)\n",
    "batch_size = 32\n",
    "num_epochs = 300\n",
    "learning_rate = 1e-3\n",
    "loss_function = 'mse'\n",
    "area_normalize = True\n",
    "model_overwrite = True\n",
    "\n",
    "LR_SCHEDULE = 'cosine'\n",
    "WARMUP_EPOCHS = 5\n",
    "MIN_LEARNING_RATE = 1e-5\n",
    "\n",
    "CSV_ID_COL = 'id'\n",
    "CSV_LAT_COL = 'latitude'\n",
    "CSV_LON_COL = 'longitude'\n",
    "CSV_DATE_COL = 'date'\n",
    "CSV_DISCHARGE_COL = 'discharge'\n",
    "CSV_FILE_PATTERN = '{id}.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c97da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "working_dir_drive = Path('/content/drive/MyDrive') / folder_name\n",
    "working_dir_local = Path('/content') / folder_name\n",
    "working_dir = working_dir_drive\n",
    "study_area = working_dir_drive / 'shapes' / shapefile_name\n",
    "\n",
    "if OBSERVED_DATA_MODE == 'GRDC':\n",
    "    grdc_netcdf = working_dir_drive / 'data' / grdc_filename\n",
    "    csv_dir = None\n",
    "    lookup_csv = None\n",
    "elif OBSERVED_DATA_MODE == 'CSV':\n",
    "    grdc_netcdf = None\n",
    "    csv_dir = working_dir_drive / 'data' / csv_dir_name\n",
    "    lookup_csv = working_dir_drive / 'data' / lookup_csv_name\n",
    "else:\n",
    "    raise ValueError(\"OBSERVED_DATA_MODE must be 'GRDC' or 'CSV'.\")\n",
    "\n",
    "working_dir_drive.mkdir(parents=True, exist_ok=True)\n",
    "(working_dir_drive / 'shapes').mkdir(parents=True, exist_ok=True)\n",
    "(working_dir_drive / 'data').mkdir(parents=True, exist_ok=True)\n",
    "working_dir_local.mkdir(parents=True, exist_ok=True)\n",
    "(working_dir_local / 'shapes').mkdir(parents=True, exist_ok=True)\n",
    "(working_dir_local / 'data').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('working_dir (drive):', working_dir_drive)\n",
    "print('working_dir (local):', working_dir_local)\n",
    "print('study_area:', study_area)\n",
    "print('mode:', OBSERVED_DATA_MODE)\n",
    "print('grdc_netcdf:', grdc_netcdf)\n",
    "print('csv_dir:', csv_dir)\n",
    "print('lookup_csv:', lookup_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6b3e2c",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Beginner run order:\n",
    "1. Edit the **User Inputs** cell.\n",
    "2. (Optional) edit **Optional Advanced Settings**.\n",
    "3. Run setup/validation, then run sections top-to-bottom.\n",
    "\n",
    "Observed-data mode:\n",
    "- `OBSERVED_DATA_MODE = 'GRDC'`: set `grdc_filename`\n",
    "- `OBSERVED_DATA_MODE = 'CSV'`: set `csv_dir_name` and `lookup_csv_name`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce1f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not study_area.exists():\n",
    "    raise FileNotFoundError(f'study_area not found: {study_area}')\n",
    "\n",
    "grdc_mode = grdc_netcdf is not None\n",
    "csv_mode = (csv_dir is not None) and (lookup_csv is not None)\n",
    "\n",
    "if grdc_mode == csv_mode:\n",
    "    raise ValueError(\n",
    "        'Choose exactly one observed-data mode:\\n'\n",
    "        \"  1) GRDC mode: set grdc_netcdf=Path(...), csv_dir=None, lookup_csv=None\\n\"\n",
    "        \"  2) CSV mode: set grdc_netcdf=None, csv_dir=Path(...), lookup_csv=Path(...)\"\n",
    "    )\n",
    "\n",
    "grdc_netcdf_runtime = None\n",
    "\n",
    "def open_grdc_with_fallback(nc_path):\n",
    "    import xarray as xr\n",
    "    errors = []\n",
    "    for engine in [None, 'h5netcdf']:\n",
    "        try:\n",
    "            if engine is None:\n",
    "                return xr.open_dataset(nc_path), 'netcdf4(default)'\n",
    "            return xr.open_dataset(nc_path, engine=engine), engine\n",
    "        except Exception as e:\n",
    "            name = 'netcdf4(default)' if engine is None else engine\n",
    "            errors.append(f'{name}: {e}')\n",
    "    raise RuntimeError('Unable to open GRDC NetCDF with available backends:\\n' + '\\n'.join(errors))\n",
    "\n",
    "if grdc_mode:\n",
    "    if not grdc_netcdf.exists():\n",
    "        raise FileNotFoundError(f'grdc_netcdf not found: {grdc_netcdf}')\n",
    "\n",
    "    src_size = grdc_netcdf.stat().st_size\n",
    "    if src_size <= 0:\n",
    "        raise RuntimeError(f'GRDC file is empty: {grdc_netcdf}')\n",
    "\n",
    "    usage = shutil.disk_usage('/content')\n",
    "    if usage.free < src_size + 200 * 1024 * 1024:\n",
    "        raise RuntimeError(\n",
    "            'Not enough free space in /content to stage GRDC NetCDF. '\n",
    "            f'Need at least {src_size + 200 * 1024 * 1024} bytes.'\n",
    "        )\n",
    "\n",
    "    grdc_netcdf_runtime = working_dir_local / 'grdc' / grdc_netcdf.name\n",
    "    grdc_netcdf_runtime.parent.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy2(grdc_netcdf, grdc_netcdf_runtime)\n",
    "\n",
    "    dst_size = grdc_netcdf_runtime.stat().st_size\n",
    "    if dst_size != src_size:\n",
    "        raise RuntimeError(\n",
    "            'GRDC file copy size mismatch; copy may be incomplete. '\n",
    "            f'source={src_size} bytes, copied={dst_size} bytes'\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        ds_chk, engine_used = open_grdc_with_fallback(grdc_netcdf_runtime)\n",
    "        with ds_chk:\n",
    "            _ = tuple(ds_chk.dims.keys())\n",
    "        print('GRDC runtime copy ready:', grdc_netcdf_runtime)\n",
    "        print('GRDC backend:', engine_used)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            f'Failed to open GRDC NetCDF after local copy: {grdc_netcdf_runtime}\\n'\n",
    "            'If this file opens on HPC but not on Colab, install h5netcdf and retry:\\n'\n",
    "            '  !pip -q install h5netcdf\\n'\n",
    "            f'Details: {e}'\n",
    "        ) from e\n",
    "\n",
    "if csv_mode:\n",
    "    if not csv_dir.exists():\n",
    "        raise FileNotFoundError(f'csv_dir not found: {csv_dir}')\n",
    "    if not lookup_csv.exists():\n",
    "        raise FileNotFoundError(f'lookup_csv not found: {lookup_csv}')\n",
    "\n",
    "print('Input validation passed.')\n",
    "print('Mode:', 'GRDC' if grdc_mode else 'CSV')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4381731",
   "metadata": {},
   "source": [
    "## 1) Download and preprocess input data\n",
    "\n",
    "Can run independently: **Yes**, after setup/configuration cells are run.\n",
    "\n",
    "Required user data:\n",
    "- `study_area` shapefile in `working_dir_drive/shapes/`\n",
    "\n",
    "Required configuration:\n",
    "- `RUN_PREPROCESS=True`\n",
    "- `climate_data_source` (`ERA5`, `CHIRPS`, or `CHELSA`)\n",
    "- preprocessing dates in the code cell below\n",
    "\n",
    "Recommended order used here:\n",
    "1. DEM first (creates reference grid `elevation/dem_clipped.tif`)\n",
    "2. Tree cover\n",
    "3. NDVI\n",
    "4. Soil\n",
    "5. AlphaEarth\n",
    "6. Meteorology\n",
    "\n",
    "Dataset availability:\n",
    "- Tree cover (MODIS VCF): 2001 onward\n",
    "- NDVI (MODIS 16-day): 2001 onward\n",
    "- AlphaEarth embeddings: 2017 onward\n",
    "\n",
    "Expected outputs:\n",
    "- `elevation/`, `vcf/`, `ndvi/`, `soil/`, `alpha_earth/`\n",
    "- climate NetCDF files under `working_dir_drive/{climate_data_source}/`\n",
    "\n",
    "Resume behavior:\n",
    "- Reruns reuse existing outputs and process/download only missing pieces where supported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e94ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "ee.Authenticate(auth_mode=\"notebook\")   # forces link + paste code flow\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d20ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_PREPROCESS:\n",
    "    from bakaano.dem import DEM\n",
    "    from bakaano.tree_cover import TreeCover\n",
    "    from bakaano.ndvi import NDVI\n",
    "    from bakaano.soil import Soil\n",
    "    from bakaano.alpha_earth import AlphaEarth\n",
    "    from bakaano.meteo import Meteo\n",
    "\n",
    "    dd = DEM(\n",
    "        working_dir=str(working_dir),\n",
    "        study_area=str(study_area),\n",
    "        local_data=False,\n",
    "        local_data_path=None,\n",
    "    )\n",
    "    dd.get_dem_data()\n",
    "\n",
    "    vf = TreeCover(\n",
    "        working_dir=str(working_dir),\n",
    "        study_area=str(study_area),\n",
    "        start_date=WORKFLOW_START_DATE,\n",
    "        end_date=WORKFLOW_END_DATE,\n",
    "    )\n",
    "    vf.get_tree_cover_data()\n",
    "\n",
    "    nd = NDVI(\n",
    "        working_dir=str(working_dir),\n",
    "        study_area=str(study_area),\n",
    "        start_date=WORKFLOW_START_DATE,\n",
    "        end_date=WORKFLOW_END_DATE,\n",
    "    )\n",
    "    nd.get_ndvi_data()\n",
    "\n",
    "    sgd = Soil(\n",
    "        working_dir=str(working_dir),\n",
    "        study_area=str(study_area),\n",
    "    )\n",
    "    sgd.get_soil_data()\n",
    "\n",
    "    ae = AlphaEarth(\n",
    "        working_dir=str(working_dir),\n",
    "        study_area=str(study_area),\n",
    "        start_date=WORKFLOW_START_DATE,\n",
    "        end_date=WORKFLOW_END_DATE,\n",
    "    )\n",
    "    ae.get_alpha_earth()\n",
    "\n",
    "    cd = Meteo(\n",
    "        working_dir=str(working_dir_local),\n",
    "        study_area=str(study_area),\n",
    "        start_date=WORKFLOW_START_DATE,\n",
    "        end_date=WORKFLOW_END_DATE,\n",
    "        local_data=False,\n",
    "        data_source=climate_data_source,\n",
    "    )\n",
    "    cd.get_meteo_data()\n",
    "    src_climate = working_dir_local / climate_data_source\n",
    "    dst_climate = working_dir_drive / climate_data_source\n",
    "    if src_climate.exists():\n",
    "        shutil.copytree(src_climate, dst_climate, dirs_exist_ok=True)\n",
    "        print(f'Synced climate outputs to Drive: {dst_climate}')\n",
    "\n",
    "    print('Preprocessing complete.')\n",
    "else:\n",
    "    print('Skipping preprocessing step.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e992bd2e",
   "metadata": {},
   "source": [
    "### Optional quick plots\n",
    "\n",
    "Run any of these in a new code cell if needed:\n",
    "- `vf.plot_tree_cover(variable='tree_cover')`\n",
    "- `nd.plot_ndvi(interval_num=10)`\n",
    "- `dd.plot_dem()`\n",
    "- `sgd.plot_soil(variable='wilting_point')`\n",
    "- `ae.plot_alpha_earth('A35')`\n",
    "- `cd.plot_meteo(variable='tasmin', date='2006-12-01')`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba5c85e",
   "metadata": {},
   "source": [
    "## 2) Compute runoff and route to river network\n",
    "\n",
    "Can run independently: **Yes**, if preprocessing outputs already exist in Drive.\n",
    "\n",
    "Required user data:\n",
    "- basin shapefile in `working_dir_drive/shapes/`\n",
    "\n",
    "Required existing preprocessed inputs in Drive:\n",
    "- `elevation/`, `soil/`, `ndvi/`, `vcf/`, `alpha_earth/`, and `{climate_data_source}/`\n",
    "\n",
    "Required configuration:\n",
    "- `RUN_RUNOFF_ROUTING=True`\n",
    "- `routing_method` (`mfd`, `d8`, or `dinf`)\n",
    "- runoff dates in the code cell below\n",
    "\n",
    "Execution behavior:\n",
    "- Inputs are staged to local runtime (`/content/...`) for speed\n",
    "- VegET runs locally\n",
    "- `runoff_output/` and `catchment/` are synced back to Drive\n",
    "\n",
    "Expected outputs:\n",
    "- `runoff_output/wacc_sparse_arrays.pkl`\n",
    "- routed runoff outputs under `runoff_output/`\n",
    "\n",
    "Resume behavior:\n",
    "- If interrupted, rerun this section; checkpoint files allow continuation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca5a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if RUN_RUNOFF_ROUTING:\n",
    "    from bakaano.veget import VegET\n",
    "\n",
    "    for folder in ['shapes', 'elevation', 'soil', 'ndvi', 'vcf', 'alpha_earth', climate_data_source]:\n",
    "        src = working_dir_drive / folder\n",
    "        dst = working_dir_local / folder\n",
    "        if src.exists():\n",
    "            shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "    local_study_area = working_dir_local / 'shapes' / shapefile_name\n",
    "\n",
    "    vg = VegET(\n",
    "        working_dir=str(working_dir_local),\n",
    "        study_area=str(local_study_area),\n",
    "        start_date=WORKFLOW_START_DATE,\n",
    "        end_date=WORKFLOW_END_DATE,\n",
    "        climate_data_source=climate_data_source,\n",
    "        routing_method=routing_method,\n",
    "    )\n",
    "    vg.compute_veget_runoff_route_flow()\n",
    "    for folder in ['runoff_output', 'catchment']:\n",
    "        src = working_dir_local / folder\n",
    "        dst = working_dir_drive / folder\n",
    "        if src.exists():\n",
    "            shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "    print('Runoff and routing complete.')\n",
    "else:\n",
    "    print('Skipping runoff/routing step.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bf49cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bakaano.plot_runoff import RoutedRunoff\n",
    "\n",
    "rr = RoutedRunoff(\n",
    "    working_dir=str(working_dir),\n",
    "    study_area=str(study_area),\n",
    ")\n",
    "\n",
    "rr.map_routed_runoff(date='2003-07-07', vmax=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bdbb93",
   "metadata": {},
   "source": [
    "## 3) Interactive exploration\n",
    "\n",
    "Can run independently: **Yes**, if required raster/runoff outputs already exist.\n",
    "\n",
    "Required user data:\n",
    "- `study_area` shapefile\n",
    "- optional GRDC NetCDF (for station overlays)\n",
    "\n",
    "Required existing outputs:\n",
    "- `elevation/dem_clipped.tif`\n",
    "- `soil/`, `vcf/`, `elevation/slope_clipped.tif`\n",
    "- runoff/catchment outputs from Section 2\n",
    "\n",
    "Required configuration:\n",
    "- `grdc_netcdf_runtime` available for GRDC station overlays\n",
    "- map date ranges in the code cell below\n",
    "\n",
    "Typical checks:\n",
    "- inspect DEM/tree cover/soil/river network layers\n",
    "- verify station coverage and missingness\n",
    "- confirm runoff outputs look reasonable before training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8732b41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from bakaano.runner import BakaanoHydro\n",
    "\n",
    "bk = BakaanoHydro(\n",
    "    working_dir=str(working_dir),\n",
    "    study_area=str(study_area),\n",
    "    climate_data_source=climate_data_source,\n",
    ")\n",
    "\n",
    "if grdc_netcdf_runtime is not None and grdc_netcdf_runtime.exists():\n",
    "    explore_map = bk.explore_data_interactively(\n",
    "        start_date=WORKFLOW_START_DATE,\n",
    "        end_date=WORKFLOW_END_DATE,\n",
    "        grdc_netcdf=str(grdc_netcdf_runtime),\n",
    "    )\n",
    "    display(explore_map)\n",
    "else:\n",
    "    print('Skipping explore_data_interactively (no GRDC NetCDF path set).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cac1a82",
   "metadata": {},
   "source": [
    "### Routed runoff timeseries\n",
    "\n",
    "This section is fully interactive.\n",
    "It lists available station IDs in your study area and prompts you to enter one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305a854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if grdc_netcdf_runtime is not None and grdc_netcdf_runtime.exists():\n",
    "    rr.interactive_plot_routed_runoff_timeseries(\n",
    "        start_date=WORKFLOW_START_DATE,\n",
    "        end_date=WORKFLOW_END_DATE,\n",
    "        grdc_netcdf=str(grdc_netcdf_runtime),\n",
    "    )\n",
    "elif lookup_csv is not None and lookup_csv.exists():\n",
    "    rr.interactive_plot_routed_runoff_timeseries(\n",
    "        start_date=WORKFLOW_START_DATE,\n",
    "        end_date=WORKFLOW_END_DATE,\n",
    "        lookup_csv=str(lookup_csv),\n",
    "        id_col=CSV_ID_COL,\n",
    "        lat_col=CSV_LAT_COL,\n",
    "        lon_col=CSV_LON_COL,\n",
    "    )\n",
    "else:\n",
    "    print('Set grdc_netcdf or lookup_csv before running interactive routed runoff timeseries.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ac3e76",
   "metadata": {},
   "source": [
    "## 4) Train model\n",
    "\n",
    "Can run independently: **Yes**, if runoff/catchment outputs and observed data are ready.\n",
    "\n",
    "Required user data (choose one mode):\n",
    "- GRDC mode: `grdc_netcdf` file\n",
    "- CSV mode: `csv_dir` + `lookup_csv`\n",
    "\n",
    "Required existing outputs:\n",
    "- Section 2 outputs in `runoff_output/` and `catchment/`\n",
    "- predictor rasters from preprocessing\n",
    "\n",
    "Required configuration:\n",
    "- `RUN_TRAIN=True`\n",
    "- `batch_size`, `num_epochs`, `learning_rate`, `loss_function`\n",
    "- `routing_method`, `area_normalize`, train date range\n",
    "- `model_overwrite`\n",
    "\n",
    "Training control:\n",
    "- `model_overwrite=True`: train fresh model\n",
    "- `model_overwrite=False`: continue from existing model if present\n",
    "\n",
    "Expected output:\n",
    "- `models/bakaano_model.keras`\n",
    "\n",
    "GRDC backend note:\n",
    "- Notebook stages GRDC NetCDF to local runtime and uses backend fallback (`netcdf4`/`h5netcdf`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93e3503",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_TRAIN:\n",
    "    if grdc_netcdf_runtime is not None and grdc_netcdf_runtime.exists():\n",
    "        bk.train_streamflow_model(\n",
    "            train_start=TRAIN_START_DATE,\n",
    "            train_end=TRAIN_END_DATE,\n",
    "            grdc_netcdf=str(grdc_netcdf_runtime),\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            loss_function=loss_function,\n",
    "            lr_schedule=LR_SCHEDULE,\n",
    "            warmup_epochs=WARMUP_EPOCHS,\n",
    "            min_learning_rate=MIN_LEARNING_RATE,\n",
    "            routing_method=routing_method,\n",
    "            area_normalize=area_normalize,\n",
    "            model_overwrite=model_overwrite,\n",
    "        )\n",
    "    else:\n",
    "        bk.train_streamflow_model(\n",
    "            train_start=TRAIN_START_DATE,\n",
    "            train_end=TRAIN_END_DATE,\n",
    "            grdc_netcdf=None,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            loss_function=loss_function,\n",
    "            lr_schedule=LR_SCHEDULE,\n",
    "            warmup_epochs=WARMUP_EPOCHS,\n",
    "            min_learning_rate=MIN_LEARNING_RATE,\n",
    "            routing_method=routing_method,\n",
    "            area_normalize=area_normalize,\n",
    "            model_overwrite=model_overwrite,\n",
    "            csv_dir=str(csv_dir),\n",
    "            lookup_csv=str(lookup_csv),\n",
    "            id_col=CSV_ID_COL,\n",
    "            lat_col=CSV_LAT_COL,\n",
    "            lon_col=CSV_LON_COL,\n",
    "            date_col=CSV_DATE_COL,\n",
    "            discharge_col=CSV_DISCHARGE_COL,\n",
    "            file_pattern=CSV_FILE_PATTERN,\n",
    "        )\n",
    "else:\n",
    "    print('Skipping training.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33df53b",
   "metadata": {},
   "source": [
    "## 5) Evaluate model\n",
    "\n",
    "Can run independently: **Yes**, if a trained model file exists.\n",
    "\n",
    "Required user data (choose one mode):\n",
    "- GRDC mode: `grdc_netcdf` file\n",
    "- CSV mode: `csv_dir` + `lookup_csv`\n",
    "\n",
    "Required existing outputs:\n",
    "- `models/bakaano_model.keras`\n",
    "- runoff/catchment/predictor data used by the model\n",
    "\n",
    "Required configuration:\n",
    "- `RUN_EVAL=True`\n",
    "- validation period (`val_start`, `val_end`)\n",
    "- `routing_method`, `area_normalize`\n",
    "\n",
    "Expected outputs:\n",
    "- evaluation plots and metrics for observed vs predicted flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c221a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = working_dir / 'models' / 'bakaano_model.keras'\n",
    "print('model_path exists:', model_path.exists())\n",
    "\n",
    "if RUN_EVAL and model_path.exists():\n",
    "    if grdc_netcdf_runtime is not None and grdc_netcdf_runtime.exists():\n",
    "        bk.evaluate_streamflow_model_interactively(\n",
    "            model_path=str(model_path),\n",
    "            val_start=EVAL_START_DATE,\n",
    "            val_end=EVAL_END_DATE,\n",
    "            grdc_netcdf=str(grdc_netcdf_runtime),\n",
    "            routing_method=routing_method,\n",
    "            area_normalize=area_normalize,\n",
    "        )\n",
    "    else:\n",
    "        bk.evaluate_streamflow_model_interactively(\n",
    "            model_path=str(model_path),\n",
    "            val_start=EVAL_START_DATE,\n",
    "            val_end=EVAL_END_DATE,\n",
    "            grdc_netcdf=None,\n",
    "            routing_method=routing_method,\n",
    "            area_normalize=area_normalize,\n",
    "            csv_dir=str(csv_dir),\n",
    "            lookup_csv=str(lookup_csv),\n",
    "            id_col=CSV_ID_COL,\n",
    "            lat_col=CSV_LAT_COL,\n",
    "            lon_col=CSV_LON_COL,\n",
    "            date_col=CSV_DATE_COL,\n",
    "            discharge_col=CSV_DISCHARGE_COL,\n",
    "            file_pattern=CSV_FILE_PATTERN,\n",
    "        )\n",
    "else:\n",
    "    print('Skipping evaluation.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5041ce1",
   "metadata": {},
   "source": [
    "## 6) Simulate streamflow\n",
    "\n",
    "Can run independently: **Yes**, if a trained model file exists.\n",
    "\n",
    "Required user data:\n",
    "- for station simulation: GRDC NetCDF or CSV station data\n",
    "- for point simulation: coordinate list in code cell\n",
    "\n",
    "Required existing outputs:\n",
    "- `models/bakaano_model.keras`\n",
    "- runoff/catchment/predictor data\n",
    "\n",
    "Required configuration:\n",
    "- `RUN_SIM_GRDC` and/or `RUN_SIM_POINTS`\n",
    "- simulation period (`sim_start`, `sim_end`)\n",
    "- `routing_method`, `area_normalize`\n",
    "\n",
    "Expected outputs:\n",
    "- simulated discharge series and prediction artifacts under `predicted_streamflow_data/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d8bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_path.exists() and RUN_SIM_GRDC:\n",
    "    if grdc_netcdf_runtime is not None and grdc_netcdf_runtime.exists():\n",
    "        bk.simulate_grdc_csv_stations(\n",
    "            model_path=str(model_path),\n",
    "            sim_start=WORKFLOW_START_DATE,\n",
    "            sim_end=WORKFLOW_END_DATE,\n",
    "            grdc_netcdf=str(grdc_netcdf_runtime),\n",
    "            routing_method=routing_method,\n",
    "            area_normalize=area_normalize,\n",
    "        )\n",
    "    else:\n",
    "        bk.simulate_grdc_csv_stations(\n",
    "            model_path=str(model_path),\n",
    "            sim_start=WORKFLOW_START_DATE,\n",
    "            sim_end=WORKFLOW_END_DATE,\n",
    "            grdc_netcdf=None,\n",
    "            routing_method=routing_method,\n",
    "            area_normalize=area_normalize,\n",
    "            csv_dir=str(csv_dir),\n",
    "            lookup_csv=str(lookup_csv),\n",
    "            id_col=CSV_ID_COL,\n",
    "            lat_col=CSV_LAT_COL,\n",
    "            lon_col=CSV_LON_COL,\n",
    "            date_col=CSV_DATE_COL,\n",
    "            discharge_col=CSV_DISCHARGE_COL,\n",
    "            file_pattern=CSV_FILE_PATTERN,\n",
    "        )\n",
    "else:\n",
    "    print('Skipping station simulation.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36cb632",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIM_POINT_LATLIST = [13.8, 13.9] #user provided latitudes for point simulation\n",
    "SIM_POINT_LONLIST = [3.0, 4.0] #user provided longitudes for point simulation\n",
    "\n",
    "if model_path.exists() and RUN_SIM_POINTS:\n",
    "    bk.simulate_streamflow(\n",
    "        model_path=str(model_path),\n",
    "        sim_start=WORKFLOW_START_DATE,\n",
    "        sim_end=WORKFLOW_END_DATE,\n",
    "        latlist=SIM_POINT_LATLIST,\n",
    "        lonlist=SIM_POINT_LONLIST,\n",
    "        routing_method=routing_method,\n",
    "        area_normalize=area_normalize,\n",
    "    )\n",
    "else:\n",
    "    print('Skipping point simulation.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5063c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "pred_files = sorted(glob.glob(str(working_dir / 'predicted_streamflow_data' / '*.csv')))\n",
    "print('Prediction files:', len(pred_files))\n",
    "if pred_files:\n",
    "    print('Example:', pred_files[0])\n",
    "    df = pd.read_csv(pred_files[0])\n",
    "    display(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
